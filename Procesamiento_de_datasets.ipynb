{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento de datasets\n",
    "En este notebook están agrupados los procedimientos por los cuales se importaron, concatenaron y procesaron los sets de datos originalmente provistos:\n",
    "- `dataset_es_dev.json`\n",
    "- `dataset_es_train.json`\n",
    "- `dataset_es_test.json`\n",
    "\n",
    "Para generar sets de datos con los atributos relevantes y las reviews lemmatizadas y stemmatizadas, segpun el caso:\n",
    "- `dataset_amazon_reviews_lemma.json`\n",
    "- `dataset_amazon_reviews_stem.json`\n",
    "\n",
    "De esta forma, se ahorra tiempo en la ejecución del notebook que resulve la consigna pedida.\n",
    "\n",
    "### Importar las librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Garbage collector para optimizar recursos\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previamente\n",
    "## python -m spacy download es\n",
    "## python -m spacy download es_core_news_sm\n",
    "\n",
    "import spacy # https://spacy.io/usage/models\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "#Stop Words de es_core_news_sm\n",
    "from spacy.lang.es.stop_words import STOP_WORDS\n",
    "stopwords_spacy = list(STOP_WORDS)\n",
    "\n",
    "# Quitamos de las stopwords palabras como 'no', 'bueno', 'buena' que nos pueden ser últiles para predecir\n",
    "stopwords_spacy.remove('no') # En los comentarios negativos es muy repetida\n",
    "stopwords_spacy.remove('bueno')\n",
    "stopwords_spacy.remove('buena')\n",
    "stopwords_spacy.remove('tarde')\n",
    "stopwords_spacy.remove('temprano')\n",
    "stopwords_spacy.remove('día')\n",
    "stopwords_spacy.remove('días')\n",
    "stopwords_spacy.remove('dia')\n",
    "stopwords_spacy.remove('dias')\n",
    "stopwords_spacy.remove('grandes')\n",
    "stopwords_spacy.remove('general')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "#Stop Words de nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords_nltk = set(stopwords.words('spanish'))\n",
    "\n",
    "# Quitamos de las stopwords la palabra 'no', que nos puede ser últil para predecir\n",
    "stopwords_nltk.remove('no')\n",
    "\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clase para personalizar las impresiones de consola\n",
    "class color:\n",
    "   PURPLE = '\\033[95m'\n",
    "   CYAN = '\\033[96m'\n",
    "   DARKCYAN = '\\033[36m'\n",
    "   BLUE = '\\033[94m'\n",
    "   GREEN = '\\033[92m'\n",
    "   YELLOW = '\\033[93m'\n",
    "   RED = '\\033[91m'\n",
    "   BOLD = '\\033[1m'\n",
    "   UNDERLINE = '\\033[4m'\n",
    "   END = '\\033[0m'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importar y concatenar los datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos dataset de desarrollo\n",
    "data_dev = pd.read_json(\"dataset_amazon/dataset_es_dev.json\", lines = True)\n",
    "print(\"- Cantidad de filas del set de \" + color.YELLOW + \"desarrollo\" + color.END + \":\", data_dev.shape[0])\n",
    "\n",
    "# Importamos dataset de entrenamiento\n",
    "data_train = pd.read_json(\"dataset_amazon/dataset_es_train.json\", lines = True)\n",
    "print(\"- Cantidad de filas del set de \" + color.CYAN + \"entrenamiento\" + color.END + \":\", data_train.shape[0])\n",
    "\n",
    "# Importamos dataset de prueba\n",
    "data_test = pd.read_json(\"dataset_amazon/dataset_es_test.json\", lines = True)\n",
    "print(\"- Cantidad de filas del set de \" + color.GREEN + \"prueba\" + color.END + \":\", data_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenamos todos\n",
    "data = pd.concat([data_dev,data_train,data_test])\n",
    "data = data.reset_index(drop=True)\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liberamos espacio de memoria\n",
    "del(data_dev)\n",
    "del(data_train)\n",
    "del(data_test)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtrado de columnas\n",
    "Nos quedamos con las columnas del dataset que son relevantes para predecir la valoración en general. Por eso se descartan las columnas de id de usuario y producto. La de categoría nos puede servir para relacionar palabras. La del idioma es redundante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data[['product_category','review_title','review_body','stars']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpieza de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constante de signos de puntuación\n",
    "import string\n",
    "puntua = string.punctuation + '¡¿...'\n",
    "excluded_pos = ['SCONJ','CCONJ','NUM','PUNCT','PRON','DET','ADP','AUX','X']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Función para limpieza de datos con lemmatizer\n",
    "def text_data_lemma(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    clean_tokens = []\n",
    "    for token in doc:\n",
    "        if (token.pos_ not in excluded_pos and str(token) not in stopwords_spacy and len(token.text)>2): \n",
    "            temp = token.lemma_.strip()\n",
    "            clean_tokens.append(temp.lower())\n",
    "    \n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SpanishStemmer # Permite stemmizar palabras en español\n",
    "stemmer = SpanishStemmer()\n",
    "\n",
    "#Función para limpieza de datos con stemmer\n",
    "def text_data_stem(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    clean_tokens = []\n",
    "    for token in doc:\n",
    "        if (token.pos_ not in excluded_pos and str(token) not in stopwords_spacy and len(token.text)>2): \n",
    "            temp = stemmer.stem(token.text).strip()\n",
    "            clean_tokens.append(temp.lower())\n",
    "    \n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatización (40 min aprox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpiamos todas las reviews con lemmatizer\n",
    "reviews_lemma = []\n",
    "for i in df.index:\n",
    "    rev = text_data_lemma(df.review_title.iloc[i] + ' ' + df.review_body.iloc[i])\n",
    "    reviews_lemma.append(\" \".join(rev))\n",
    "reviews_lemma[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemmización (40 min aprox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpiamos todas las reviews con stemmizer\n",
    "reviews_stem = []\n",
    "for i in df.index:\n",
    "    rev = text_data_stem(df.review_title.iloc[i] + ' ' + df.review_body.iloc[i])\n",
    "    reviews_stem.append(\" \".join(rev))\n",
    "reviews_stem[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar nuevos datos en achivo json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregamos columna al dataset\n",
    "df['revs_lemma'] = reviews_lemma\n",
    "df['revs_stem'] = reviews_stem\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos dataset lemmatizado\n",
    "df.to_json(path_or_buf='dataset_amazon_clean.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
